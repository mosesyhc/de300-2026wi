[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DATA_ENG 300: Data Engineering Studio",
    "section": "",
    "text": "Course Outline\nInstructor: Moses Chan, moses.chan@northwestern.edu\nTA: Dinglin Xia\nClassroom: Lecture: 2122 Sheridan Rm 250, Lab: Tech C135\nOffice Hours: See Canvas.\nPrerequisites: DATA_ENG 200, Statistics Foundations course; CS150; CS 214 or 217; IEMS 304 or CS 349.\nIntroduction to the foundation and modern tools of data engineering. Topics include:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>DATA_ENG 300: Data Engineering Studio</span>"
    ]
  },
  {
    "objectID": "index.html#inclusivity",
    "href": "index.html#inclusivity",
    "title": "DATA_ENG 300: Data Engineering Studio",
    "section": "Inclusivity",
    "text": "Inclusivity\nThis course strives to be an inclusive learning community, respecting those of differing backgrounds and beliefs. As a community, we aim to be respectful to all students in this class, regardless of race, ethnicity, socio-economic status, religion, gender identity or sexual orientation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>DATA_ENG 300: Data Engineering Studio</span>"
    ]
  },
  {
    "objectID": "index.html#office-hours",
    "href": "index.html#office-hours",
    "title": "DATA_ENG 300: Data Engineering Studio",
    "section": "Office Hours",
    "text": "Office Hours\nWeekly office hours are a dedicated time that the teaching team is available to answer your questions, discuss course content, and generally be of support. If none of the offered times in the syllabus works for you, please email the instructor to schedule an appointment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>DATA_ENG 300: Data Engineering Studio</span>"
    ]
  },
  {
    "objectID": "index.html#late-days",
    "href": "index.html#late-days",
    "title": "DATA_ENG 300: Data Engineering Studio",
    "section": "Late Days",
    "text": "Late Days\nYou have 4 late days total for the quarter that you may use on homeworks. Late days are automatically applied.\nA late day covers up to 24 hours after the deadline (or any part of that 24-hour period). Submissions covered by available late days are accepted with no penalty. No assignment is accepted more than 48 hours late, i.e., at most 2 late days can be used on one assignment.\nIf you have no late days remaining, late work will incur a penalty:\n\nLess than 24 hours late: 20% deduction\n24–48 hours late: 40% deduction\nMore than 48 hours late: not accepted\n\nPlease contact Professor Chan in the case of an emergency and discuss a plan.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>DATA_ENG 300: Data Engineering Studio</span>"
    ]
  },
  {
    "objectID": "index.html#communications",
    "href": "index.html#communications",
    "title": "DATA_ENG 300: Data Engineering Studio",
    "section": "Communications",
    "text": "Communications\nCanvas, where the syllabus, class announcements, exams, notes, and assignments are posted.\nEmails should lead with a title including DE300, followed by the matter of concern. Formal communications should be directed to emails, e.g., accommodation, progress discussion. The teaching team observes a 24-workhour response policy.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>DATA_ENG 300: Data Engineering Studio</span>"
    ]
  },
  {
    "objectID": "index.html#collaboration",
    "href": "index.html#collaboration",
    "title": "DATA_ENG 300: Data Engineering Studio",
    "section": "Collaboration",
    "text": "Collaboration\nYou are encouraged to work with classmates on homeworks and labs. Discussing ideas, comparing approaches, and helping each other debug are all part of learning.\nYou are required to submit your own homeworks and labs written in your own words and presented in your own ways. Do not copy or paraphrase any other write-up other than your own, or share code files and solutions to others intended for direct usage.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>DATA_ENG 300: Data Engineering Studio</span>"
    ]
  },
  {
    "objectID": "index.html#generative-ai-policy",
    "href": "index.html#generative-ai-policy",
    "title": "DATA_ENG 300: Data Engineering Studio",
    "section": "Generative AI Policy",
    "text": "Generative AI Policy\nGenAI tools (e.g., ChatGPT, Copilot) may be used to support learning and productivity, but you are considered the engineer of record: you are fully responsible for the correctness, reproducibility, and integrity of all work you submit, and you must be able to explain and justify your design choices (e.g., schema/modeling, data processing choices, cost/performance, security/privacy).\nAllowed uses: brainstorming pipeline designs and tradeoffs; debugging; drafting documentation; generating small code snippets that you can adapt and verify; identifying strengths and weaknesses of code.\nRequired disclosure: each submission must include an AI Usage note stating: (1) tool(s) used, (2) the key prompt(s), and (3) what you changed and how you verified the results. If none, write: “AI Usage: None.”\nNot allowed: submitting work you do not understand or cannot explain; fabricating results, benchmarks, or citations; sharing credentials, private data, or any restricted datasets with GenAI tools.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>DATA_ENG 300: Data Engineering Studio</span>"
    ]
  },
  {
    "objectID": "index.html#accessibility",
    "href": "index.html#accessibility",
    "title": "DATA_ENG 300: Data Engineering Studio",
    "section": "Accessibility",
    "text": "Accessibility\nNorthwestern University is committed to providing a supportive environment for students with disabilities. Should you anticipate or experience disability-related barriers in the academic setting, please contact AccessibleNU to move forward with the university’s established accommodation process. If you already have established accommodations with AccessibleNU, please let your instructor know as soon as possible, preferably within the first two weeks of the term, so we can work with you to implement your disability accommodations. Disability information, including academic accommodations, is confidential under the Family Educational Rights and Privacy Act.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>DATA_ENG 300: Data Engineering Studio</span>"
    ]
  },
  {
    "objectID": "index.html#northwestern-university-syllabus-standards",
    "href": "index.html#northwestern-university-syllabus-standards",
    "title": "DATA_ENG 300: Data Engineering Studio",
    "section": "Northwestern University Syllabus Standards",
    "text": "Northwestern University Syllabus Standards\nIn addition, this course follows the Northwestern University Syllabus Standards. Students are responsible for familiarizing themselves with this information.\n\n\n\n\nReis, Joe, and Matt Housley. 2022. Fundamentals of Data Engineering. O’Reilly Media, Inc.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>DATA_ENG 300: Data Engineering Studio</span>"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "DATA_ENG 300: Data Engineering Studio",
    "section": "",
    "text": "The lowest lab and the lowest (or missed) in-class participation will be dropped.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>DATA_ENG 300: Data Engineering Studio</span>"
    ]
  },
  {
    "objectID": "01-intro-data-engineering.html",
    "href": "01-intro-data-engineering.html",
    "title": "Introduction to Data Engineering",
    "section": "",
    "text": "Data Engineering Lifecycle\nThis notebook summarizes the data engineering lifecycle, and emphasizes the components covered in DATA_ENG 300.\nReis, J., & Housley, M. (2022) describes the data engineering lifecycle with five main stages:\n- Generation - Storage - Ingestion - Transformation - Serving data\n```",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Data Engineering</span>"
    ]
  },
  {
    "objectID": "01-intro-data-engineering.html#data-engineering-lifecycle",
    "href": "01-intro-data-engineering.html#data-engineering-lifecycle",
    "title": "Introduction to Data Engineering",
    "section": "",
    "text": "Data engineering lifecycle components (Reis, J., & Housley, M. (2022))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Data Engineering</span>"
    ]
  },
  {
    "objectID": "01-intro-data-engineering.html#preparation-for-next-week",
    "href": "01-intro-data-engineering.html#preparation-for-next-week",
    "title": "Introduction to Data Engineering",
    "section": "Preparation for next week",
    "text": "Preparation for next week\n\nLabs are on Tuesdays 5 - 7PM\nFamiliarize with Github (if you have not, register for an account.)\nFamiliarize with colab / jupyter notebook",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Data Engineering</span>"
    ]
  },
  {
    "objectID": "class-notebooks/02-exploratory-data-analysis-class.html",
    "href": "class-notebooks/02-exploratory-data-analysis-class.html",
    "title": "Intro to EDA",
    "section": "",
    "text": "Goals\nExploratory data analysis, or EDA, is a standard practice prior to any data manipulation and analysis.\nRecall that data engineering is primarily about data preparation to serve smooth and effective data analysis. Exploratory data analysis generally refers to the step of understanding the data:\nThis document primarily deals with the first two items.\nIn the exploratory phase, these are for people behind the scenes to see.\nThe main goals here are:\nWhat does this translate to, technically?",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Intro to EDA</span>"
    ]
  },
  {
    "objectID": "class-notebooks/02-exploratory-data-analysis-class.html#goals",
    "href": "class-notebooks/02-exploratory-data-analysis-class.html#goals",
    "title": "Intro to EDA",
    "section": "",
    "text": "capture main message\n(relatively) quick exploration across many summaries (including plots)\nnot intended for a client or presentation\n\n\n\neach summary should have meaningful information\nlabel your plots",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Intro to EDA</span>"
    ]
  },
  {
    "objectID": "class-notebooks/02-exploratory-data-analysis-class.html#data-summary",
    "href": "class-notebooks/02-exploratory-data-analysis-class.html#data-summary",
    "title": "Intro to EDA",
    "section": "Data summary",
    "text": "Data summary\nAs a starting point, simply looking at the data is worth the while. Some common questions to consider are the following:\n\nGeneral dataset info: size, dtypes\n\nMissing values?\n\nDuplicate data?\n\nContinuous variables\n\nCategorical variables\n\nBivariate relationships\n\nPotential data quality issues, e.g., inconsistency, special NA characters\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\n\n\n\nThe origin of sns.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Intro to EDA</span>"
    ]
  },
  {
    "objectID": "class-notebooks/02-exploratory-data-analysis-class.html#earthquake-dataset",
    "href": "class-notebooks/02-exploratory-data-analysis-class.html#earthquake-dataset",
    "title": "Intro to EDA",
    "section": "Earthquake dataset",
    "text": "Earthquake dataset\nSource Link\n\n# load and save a copy of the earthquake dataset\nearthquake = pd.read_csv('https://raw.githubusercontent.com/mosesyhc/de300-2026wi/refs/heads/main/datasets/Canadian-Earthquakes-2010-2019.csv')\n\n\n# take a glimpse of the data\n\n\n# view a summary of the full data\n\n\n# checks for duplicates (also ask if duplicates make sense)\n\n\n# duplicates\n\n\n# a quick numerical summary \n\n\n# checks for possible statistical assumption(s)\nimport scipy.stats as sps\n\n\n# extract only numeric variables\n\n\n# for example, normality test\n\n\n# for example, another normality test\n\n\n# pairwise correlation",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Intro to EDA</span>"
    ]
  },
  {
    "objectID": "class-notebooks/02-exploratory-data-analysis-class.html#data-visualization",
    "href": "class-notebooks/02-exploratory-data-analysis-class.html#data-visualization",
    "title": "Intro to EDA",
    "section": "Data visualization",
    "text": "Data visualization\n\nsns.set(context='talk', style='ticks')  # simply for aesthetics\nsns.set_palette('magma')\n%matplotlib inline \n\n# earthquake = earthquake.sample(n=500)  # (if too slow) for illustration purposes\n\n\n# histogram for continuous variables using pandas built-in plots \n\n\n# relative frequency? ...\n\n\n# histogram of masses by group\n\n\n# other types of plots\n\n\n# counts for categorical variables\n\n\n# barplots by group\n\n\n# bivariate plots\n\n\n# bivariate plots (log-log)\n\n\n# pairwise plots  (time-consuming)\n\n\n# another pairwise plot by group",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Intro to EDA</span>"
    ]
  },
  {
    "objectID": "class-notebooks/02-exploratory-data-analysis-class.html#in-class-activity",
    "href": "class-notebooks/02-exploratory-data-analysis-class.html#in-class-activity",
    "title": "Intro to EDA",
    "section": "In-class activity",
    "text": "In-class activity\nRefer to the following figure, choose two subfigures to reproduce with the earthquake dataset.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Intro to EDA</span>"
    ]
  },
  {
    "objectID": "class-notebooks/02-exploratory-data-analysis-class.html#in-case-you-need-this-jupyter-notebook-setup",
    "href": "class-notebooks/02-exploratory-data-analysis-class.html#in-case-you-need-this-jupyter-notebook-setup",
    "title": "Intro to EDA",
    "section": "(In case you need this) Jupyter notebook setup",
    "text": "(In case you need this) Jupyter notebook setup\nVisit https://docs.jupyter.org/en/latest/install/notebook-classic.html for some guidance to set up jupyter notebook.\n\nNote: These notes are adapted from a blog post on Tom’s Blog.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Intro to EDA</span>"
    ]
  },
  {
    "objectID": "class-notebooks/03-missing-data-class.html",
    "href": "class-notebooks/03-missing-data-class.html",
    "title": "Handling missing data",
    "section": "",
    "text": "Classical categories of missing data\nThis document (briefly) introduces the concept, categorization, and treatment of missing data.\nIf we know how the missing data are generated, so-called missing data mechanisms, we can take into account why data are missing in data analysis.\nA particularly useful (and wide-known) framework for missingness categorizes missingness into the following three categories (Little and Rudin, 2019):\nCaveat… the category namings are… confusing\nMCAR (missing completely at random): how data are missing is independent with any aspect of the data\nMAR (missing at random): how data are missing depends on observed quantities, so-called predictors, or independent variables - e.g., a person with certain characteristics (known) may not answer certain questions on a survey, imagine reporting salary for different socio-economic classes - e.g., for certain inputs, a simulation model may crash or produce nonsensical outputs\nMNAR (missing not at random): how data are missing depends on the unobserved quantities - e.g., a pollutant sensor can only detect particles that are \\(&gt; 0.1\\mu\\mathrm{m}\\)\n- e.g., storm surges only occur in areas that get below the water level\nUnderstanding the causes of missing data helps designing algorithms to handle missing data. However, in reality, we often do not know the causes, or multiple causes exist in producing a data set.\nCautionary note: Where the data are missing matters, i.e., is a response value missing, is a predictor value missing, or do we not know what our analysis questions are yet?",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Handling missing data</span>"
    ]
  },
  {
    "objectID": "class-notebooks/03-missing-data-class.html#treatment-of-missing-data",
    "href": "class-notebooks/03-missing-data-class.html#treatment-of-missing-data",
    "title": "Handling missing data",
    "section": "Treatment of missing data",
    "text": "Treatment of missing data\nStarting with an example of “completing the data”\n\n\n\n\n\n\n\n\n\n\n\nReplace with the mean\n\n\n\n\n\n\n\n\n\n\n\n\n\nReplace with the mean + noise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReplace with the mean + noise + parameter uncertainty\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore information + replace with mean + noise\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive mean matching\n\n\n\n\n\n\n\n\n\nExample of missingness impacting analysis from Daniel et al. (2012), reproducing Figure 4 in the paper:\n\n\n\n\n\n\n\n\nEffect of missingness mechanism on data analysis.\n\n\n\n\nGeneral rules for treating missing data:\n\nUnder MCAR, focusing on complete cases or employing imputation methods tend to be sufficient.\nUnder MAR, some methods are valid choices:\n\nIf the missingness is independent of the response conditional on the predictor, linear regression is still “valid”.\nImputation methods tend to introduce biases, depending on the data analysis methods.\n\nUnder MNAR, the missingness should be explicitly modeled.\n\ne.g., censored observations in survival analysis.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Handling missing data</span>"
    ]
  },
  {
    "objectID": "class-notebooks/03-missing-data-class.html#imputation-methods",
    "href": "class-notebooks/03-missing-data-class.html#imputation-methods",
    "title": "Handling missing data",
    "section": "Imputation methods",
    "text": "Imputation methods\nWe introduce a (limited) list of imputation methods in this section, using the titanic dataset as an example:\n\n!pip install missingno miceforest nineplot\n\nRequirement already satisfied: missingno in c:\\users\\moses\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.5.2)\nRequirement already satisfied: miceforest in c:\\users\\moses\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (6.0.5)\n\n\nERROR: Could not find a version that satisfies the requirement nineplot (from versions: none)\nERROR: No matching distribution found for nineplot\n\n[notice] A new release of pip is available: 23.3.2 -&gt; 25.3\n[notice] To update, run: python.exe -m pip install --upgrade pip\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.set_option('display.precision', 3)\n\ntitanic = sns.load_dataset('titanic', cache=True, data_home='datasets/')\n\n\nIdentifying missingness\nWe have covered some basic ways how to identify missingness in the EDA module. Here is one other option with the help of missingno.\n\n# !pip install missingno  \n# in case you need to install the package\n\nimport missingno as msno\n\n\ntitanic.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     891 non-null    int64   \n 1   pclass       891 non-null    int64   \n 2   sex          891 non-null    object  \n 3   age          714 non-null    float64 \n 4   sibsp        891 non-null    int64   \n 5   parch        891 non-null    int64   \n 6   fare         891 non-null    float64 \n 7   embarked     889 non-null    object  \n 8   class        891 non-null    category\n 9   who          891 non-null    object  \n 10  adult_male   891 non-null    bool    \n 11  deck         203 non-null    category\n 12  embark_town  889 non-null    object  \n 13  alive        891 non-null    object  \n 14  alone        891 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(4), object(5)\nmemory usage: 80.7+ KB\n\n\n\nimport missingno as msno\n# visualization of available/missing data\nmsno.bar(titanic)\nmsno.matrix(titanic)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemoving data\nWe have discussed that removing data is typically not ideal, but nothing stops us from doing that…\n\n# retaining titanic dataset with complete data\ntitanic_agecomp = titanic.dropna(subset='age', how='any')\ntitanic_agecomp.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 714 entries, 0 to 890\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype   \n---  ------       --------------  -----   \n 0   survived     714 non-null    int64   \n 1   pclass       714 non-null    int64   \n 2   sex          714 non-null    object  \n 3   age          714 non-null    float64 \n 4   sibsp        714 non-null    int64   \n 5   parch        714 non-null    int64   \n 6   fare         714 non-null    float64 \n 7   embarked     712 non-null    object  \n 8   class        714 non-null    category\n 9   who          714 non-null    object  \n 10  adult_male   714 non-null    bool    \n 11  deck         184 non-null    category\n 12  embark_town  712 non-null    object  \n 13  alive        714 non-null    object  \n 14  alone        714 non-null    bool    \ndtypes: bool(2), category(2), float64(2), int64(4), object(5)\nmemory usage: 70.2+ KB\n\n\n\n# comparing the correlation with / without missing data\ncorr = titanic.select_dtypes('number').corr()\ncorr_agecomp = titanic_agecomp.select_dtypes('number').corr()\n\n# changes in correlation matrix with / without age missingness\ncorr - corr_agecomp\n\n\n\n\n\n\n\n\nsurvived\npclass\nage\nsibsp\nparch\nfare\n\n\n\n\nsurvived\n0.000\n0.021\n0.0\n-0.018\n-0.012\n-0.011\n\n\npclass\n0.021\n0.000\n0.0\n0.016\n-0.007\n0.005\n\n\nage\n0.000\n0.000\n0.0\n0.000\n0.000\n0.000\n\n\nsibsp\n-0.018\n0.016\n0.0\n0.000\n0.031\n0.021\n\n\nparch\n-0.012\n-0.007\n0.0\n0.031\n0.000\n0.011\n\n\nfare\n-0.011\n0.005\n0.0\n0.021\n0.011\n0.000\n\n\n\n\n\n\n\n\n# one other possibility (that includes boolean)\n\n# changes in correlation matrix with / without age missingness\n\n\n\nSimple imputation with a constant\n\n# !pip install scikit-learn\nfrom sklearn.impute import SimpleImputer\n\n\ntitanic_constant = titanic.copy()\n\n# set constant, completing data\nconstant_imputer = SimpleImputer(strategy='constant')\nconstant_imputer.fill_value = 100\n\ntitanic_constant['age'] = constant_imputer.fit_transform(titanic_constant[['age']])\n\n# visualize completed data\nsns.histplot(titanic_constant['age'])\n\n\n\n\n\n\n\n\n\ntitanic_median = titanic.copy()\n\n# example with median, admitting only numeric columns\n\nmedian_imputer = SimpleImputer(strategy='median')\n\ntitanic_median['age'] = median_imputer.fit_transform(titanic_median[['age']])\n# visualize completed data\nfig, ax = plt.subplots(1, 1)\nsns.histplot(titanic_median['age'], binwidth=5, alpha=0.3, color='k', label='imputed w/ median')\nsns.histplot(titanic['age'], binwidth=5, alpha=0.3, color='purple', label='unimputed')\nax.legend()\n\n\n\n\n\n\n\n\n\n\nMultiple imputation\nMultiple imputation is a technique to generate multiple realizations of imputation values, often modeled by a probability distribution.\nOne method is the multiple imputation by chained equations, or MICE. miceforest uses LightBGM as its training algorithm for MICE.\n\n# !pip install -U miceforest plotnine\n\n\nimport miceforest as mf\n\n# miceforest requires a real matrix. \n# For illustrative purposes, we only use the numerical columns.\n\n# set up a kernel to produce `num_datasets` imputed datasets\nnum_datasets = 50\nkernel = mf.ImputationKernel(\n    titanic.select_dtypes('number'),\n    num_datasets=num_datasets,\n    save_all_iterations_data=False,\n    random_state=1\n)\n\n\n# run the MICE algorithm\nkernel.mice(iterations=10)\n\n\n\nkernel.plot_imputed_distributions()\n\n\n\n\n\n\n\n\n\n# retrieve completed data\ntitanic_mf = kernel.complete_data\n\n\n# code for plotting mice completed dataset\ndef plot_hist(kernel, completed_titanic, original_titanic, num_plot):\n    num_datasets = kernel.num_datasets\n    num_plot = min(num_datasets, num_plot)\n    for k in range(num_plot):  # 1, 2, 3, num_datasets\n        sns.histplot(completed_titanic(dataset=k)['age'], binwidth=5, stat='probability', color='k', label='imputed sample', alpha=0.3)\n    \n    sns.histplot(original_titanic['age'], binwidth=5, stat='probability', color='purple', label='unimputed', alpha=0.2)\n    \n    plt.title('{:d} realizations of imputed age'.format(num_plot))\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n\n# retrieving each imputed dataset (recall that M in MICE means multiple)\nplot_hist(kernel=kernel, completed_titanic=titanic_mf, original_titanic=titanic, num_plot=num_datasets)\n\n\n\n\n\n\n\n\n\n# comparing the original and imputed dataset\nfig, ax = plt.subplots(ncols=5)\n\nfor k, axi in enumerate(ax.flatten()):\n    if k &lt; num_datasets:\n        sns.boxplot(titanic_mf(dataset=k)['age'], ax=axi)\n    else:\n        sns.boxplot(titanic['age'], ax=axi, color='purple')\n\nplt.tight_layout()\n\nC:\\Users\\moses\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seaborn\\categorical.py:640: FutureWarning: SeriesGroupBy.grouper is deprecated and will be removed in a future version of pandas.\nC:\\Users\\moses\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seaborn\\categorical.py:640: FutureWarning: SeriesGroupBy.grouper is deprecated and will be removed in a future version of pandas.\nC:\\Users\\moses\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seaborn\\categorical.py:640: FutureWarning: SeriesGroupBy.grouper is deprecated and will be removed in a future version of pandas.\nC:\\Users\\moses\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seaborn\\categorical.py:640: FutureWarning: SeriesGroupBy.grouper is deprecated and will be removed in a future version of pandas.\nC:\\Users\\moses\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seaborn\\categorical.py:640: FutureWarning: SeriesGroupBy.grouper is deprecated and will be removed in a future version of pandas.\n\n\n\n\n\n\n\n\n\n\n# Generate more datasets \n\n\n\n# Visualize the result variable density/histogram\n\n\n\nPredictive mean matiching (MICE) [Last figure in the gas consumption example]\n\nkernel_pmm = mf.ImputationKernel(\n    titanic.select_dtypes('number'),\n    num_datasets=25,\n    mean_match_candidates=200,\n    save_all_iterations_data=False,\n    random_state=1\n)\n\n\n\nkernel_pmm.mice(iterations=10)\n\n\nkernel_pmm.plot_imputed_distributions()\n\n\n\n\n\n\n\n\n\n\n\\(k\\) Nearest Neighbor\n\ntitanic_knn = titanic.select_dtypes('number').copy()\n\nfrom sklearn.impute import KNNImputer\n# set up imputing for KNN\n\nknn_imputer = KNNImputer(n_neighbors=25)\ntitanic_knn_df = pd.DataFrame(knn_imputer.fit_transform(titanic_knn), columns=titanic_knn.columns)\n\n\n# visualize completed data\nfig, ax = plt.subplots(1, 1)\nsns.histplot(titanic_knn_df['age'], binwidth=5, stat='probability', alpha=0.3, color='k', label='knn imputation')\nsns.histplot(titanic['age'], binwidth=5, stat='probability', alpha=0.3, color='purple', label='unimputed')\nax.legend()",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Handling missing data</span>"
    ]
  },
  {
    "objectID": "class-notebooks/03-missing-data-class.html#exercise-imputation-of-tips",
    "href": "class-notebooks/03-missing-data-class.html#exercise-imputation-of-tips",
    "title": "Handling missing data",
    "section": "(Exercise) Imputation of tips",
    "text": "(Exercise) Imputation of tips\nConsider the taxis dataset from seaborn, practice the imputation methods on the tip column for the dataset subtaxi_missing, given below:\n\nComplete the data using\n\nmean simple imputation,\nmedian simple imputation,\nk-nearest neighbor, and\npredictive mean matching.\n\nCompute the root mean squared error for each imputation method (because you know the truth). Which one is the best in this case?\nExamine the correlation matrix among the numerical columns. Which methods preserve the correlation among variables?\n\n\nimport seaborn as sns\n\ntaxi = sns.load_dataset('taxis')\ntaxi.info()\nsubtaxi = taxi.loc[:, ['passengers', 'distance', 'fare', 'tip', 'tolls', 'total']]\n\n\nimport numpy as np\n# create missingness\nnp.random.seed(42)\nmask1 = subtaxi.total &gt; 25\nmask2 = np.random.rand(subtaxi.shape[0]) &lt; 0.7\n\nsubtaxi_missing = subtaxi.copy()\nsubtaxi_missing.loc[mask1 & mask2, 'tip'] = np.nan\n\n\nmsno.matrix(subtaxi_missing.sort_values('total'))",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Handling missing data</span>"
    ]
  },
  {
    "objectID": "class-notebooks/03-missing-data-class.html#exercise-imputation-of-crop-recommendation",
    "href": "class-notebooks/03-missing-data-class.html#exercise-imputation-of-crop-recommendation",
    "title": "Handling missing data",
    "section": "(Exercise) Imputation of crop recommendation",
    "text": "(Exercise) Imputation of crop recommendation\nInvestigate the dataset for crop recommendation.\n\nCreate a function that can induce some missingness (start with missing completely at random).\nCreate a logistic regression model (or any prediction model) for the column label.\nCreate a function that can take in an “imputation type” argument and return an imputed dataset.\nCompare the different imputation types in prediction accuracy of label and correlation conservation.\n\n\nimport pandas as pd\ncrop = pd.read_csv('https://raw.githubusercontent.com/mosesyhc/de300-2026wi/refs/heads/main/datasets/crop_recommendation.csv')",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Handling missing data</span>"
    ]
  },
  {
    "objectID": "04-rec-system.html",
    "href": "04-rec-system.html",
    "title": "Handling missing data in recommendation systems",
    "section": "",
    "text": "Matrix completion\nMatrix completion typically assumes some low-rank structure within the dataset.\nExamples of usage:",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Handling missing data in recommendation systems</span>"
    ]
  },
  {
    "objectID": "04-rec-system.html#matrix-completion",
    "href": "04-rec-system.html#matrix-completion",
    "title": "Handling missing data in recommendation systems",
    "section": "",
    "text": "recommendation system\nimage restoration\ngenerative image",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Handling missing data in recommendation systems</span>"
    ]
  },
  {
    "objectID": "04-rec-system.html#alternating-least-squares",
    "href": "04-rec-system.html#alternating-least-squares",
    "title": "Handling missing data in recommendation systems",
    "section": "Alternating Least Squares",
    "text": "Alternating Least Squares\nOne particular algorithm that has garnered considerable interest is the Alternating Least Squares algorithm (Hastie et al. 2015).\nSuppose we have a matrix \\(R \\in \\mathbb{R}^{n\\times m}\\) that contains user ratings for items. The matrix completion or factorziation problem typically solves for some matrices \\(X \\in \\mathbb{R}^{k\\times n}, Y \\in \\mathbb{R}^{k\\times m}\\) such that \\(R \\approx X^\\mathsf{T} Y\\), through the optimization problem\n\\[\\min_{X, Y} \\ell(X, Y) := \\underbrace{\\frac{1}{2} \\|R - X^\\mathsf{T} Y\\|^2_F}_\\text{recovery} + \\underbrace{\\frac{\\lambda}{2} (\\|X\\|^2_F + \\|Y\\|^2_F)}_\\text{regularization}.\\]\n\nObservation:\n\n\\(\\ell(X, Y)\\) is non-convex.\n\\(\\ell(X, Y)\\) is bi-convex, meaning that fixing either \\(X\\), or \\(Y\\) leads to a convex function.\n\nThe bi-convexity leads to an alternating algorithm (for a full \\(R\\)). But the missing locations do introduce nuances to the algorithm. See details in Theorem 3.1 of (Hastie et al. 2015).",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Handling missing data in recommendation systems</span>"
    ]
  },
  {
    "objectID": "04-rec-system.html#bidirectional-encoder-representations-bert-devlin2019bert",
    "href": "04-rec-system.html#bidirectional-encoder-representations-bert-devlin2019bert",
    "title": "Handling missing data in recommendation systems",
    "section": "Bidirectional Encoder Representations (BERT) (Devlin et al. 2019)",
    "text": "Bidirectional Encoder Representations (BERT) (Devlin et al. 2019)\nModern methods replace explicit low-rank assumptions with contextual representation learning, especially through masked prediction and self-attention.\nInstead of predicting a single entry or factorizing the matrix directly, consider a full row as a sequence:\n\\[\\mathbf{r}_i = (r_{i1}, r_{i2}, \\dots, r_{im})\\]\nwith missing entries treated as masked tokens.\nThe question becomes:\nhow do we predict a missing entry using all other entries in the row?\nConsider a contextual encoder (BERT):\n\\[(\\mathbf{h}_{i1}, \\dots, \\mathbf{h}_{im})=\\mathrm{BERT}_\\theta(\\mathbf{r}_i)\\]\nwhere each \\(\\mathbf{h}_{ij}\\) depends on both left and right context.\nPrediction is made from a contextual embedding, e.g.,:\n\\[\\hat{R}_{ij} = W \\mathbf{h}_{ij}\\]\nTraining is performed via masked reconstruction (randomly drop data in training):\n\\[\\min_\\theta \\underbrace{\\sum_{(i,j)\\in\\Omega_{\\text{mask}}}\\ell\\big(\\hat{R}_{ij}, R_{ij}\\big)}_{\\text{predict masked entries}}\\]\nCompare with classical matrix completion:\n\\[\\min_{X,Y}\\underbrace{\\tfrac12 \\|R - X^\\mathsf{T}Y\\|_F^2}_{\\text{low-rank structure}} + \\underbrace{\\tfrac{\\lambda}{2}(\\|X\\|_F^2+\\|Y\\|_F^2)}_{\\text{regularization}}\\]",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Handling missing data in recommendation systems</span>"
    ]
  },
  {
    "objectID": "04-rec-system.html#low-rank-adaptation-hu2022lora",
    "href": "04-rec-system.html#low-rank-adaptation-hu2022lora",
    "title": "Handling missing data in recommendation systems",
    "section": "Low rank adaptation (Hu et al. 2022)",
    "text": "Low rank adaptation (Hu et al. 2022)\nModern methods consider “pre-trained” data and adaptation to previously trained models, especially in the large language model perspective.\nInstead of viewing all data at once, consider the prediction of row \\(i\\) of the matrix \\(R\\):\n\\[ \\mathbf{r}_i = W \\underbrace{\\mathbf{e}_i}_{\\text{unit vec.}} \\]\nThe question becomes, how do we learn the weight matrix \\(W\\)?\nSuppose there is a pre-trained estimate \\(W_0\\), then matrix completion technique is used in estimating the residual:\n\\[ \\min_{X, Y} \\ell(X, Y; W_0) := \\underbrace{\\frac{1}{2} \\|R - W_0 - X^\\mathsf{T} Y\\|^2_F}_\\text{fit residual} + \\underbrace{\\frac{\\lambda}{2} (\\|X\\|^2_F + \\|Y\\|^2_F)}_\\text{regularization} \\]\nCompare with:\n\\[ \\min_{X, Y} \\ell(X, Y) := \\underbrace{\\frac{1}{2} \\|R - X^\\mathsf{T} Y\\|^2_F}_\\text{recovery} + \\underbrace{\\frac{\\lambda}{2} (\\|X\\|^2_F + \\|Y\\|^2_F)}_\\text{regularization}. \\]\n\n\n\n\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–86.\n\n\nHastie, Trevor, Rahul Mazumder, Jason D Lee, and Reza Zadeh. 2015. “Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares.” The Journal of Machine Learning Research 16 (1): 3367–3402.\n\n\nHu, Edward J, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2022. “Lora: Low-Rank Adaptation of Large Language Models.” ICLR 1 (2): 3.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Handling missing data in recommendation systems</span>"
    ]
  },
  {
    "objectID": "05-spark.html",
    "href": "05-spark.html",
    "title": "Intro to parallelized computing",
    "section": "",
    "text": "Separation of compute from storage\nThis document covers the key elements for modern distributed computing, specifically a quick word on Hadoop and Spark.\nTraditionally compute and storage are colocated, e.g.,\n. . .\nThen why separate compute and storage?\n. . .",
    "crumbs": [
      "Parallelization of computing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to parallelized computing</span>"
    ]
  },
  {
    "objectID": "05-spark.html#separation-of-compute-from-storage",
    "href": "05-spark.html#separation-of-compute-from-storage",
    "title": "Intro to parallelized computing",
    "section": "",
    "text": "a transactional database leverages its low-latency disk reads and high bandwidth.\neven for distributed jobs, a map job locates the location of data and runs the map local to the data.\n\n\n\n\nAvailability of cloud computing and storage purchase\n\nCost: It is cheap to buy and host a server than to rent if the server is up 24/7, minus maintenance cost\nTowards ephermerality: Spin up an instance, use, and stop when done.\nTowards scalability: If you only need to run this titan job once a month, only pay for the resources then.\n\n\n\n\nData durability and availability\n\nFault tolerance: Replication of data increases availability and mitgates a write operation that destroys data.\n\n\n\n\nHybridization of separation and colocation\nIn reality, compute and storage are not either separated or colocated, e.g.,\n\nAWS elastic mapreduce (EMR) and S3\nSpark infrastructure dependence on RAM\n\n\n\n\nInstances as a processing cache in an ephemeral Hadoop cluster, Fig 6.7 from Reis and Housley (2022)",
    "crumbs": [
      "Parallelization of computing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to parallelized computing</span>"
    ]
  },
  {
    "objectID": "05-spark.html#hadoop",
    "href": "05-spark.html#hadoop",
    "title": "Intro to parallelized computing",
    "section": "Hadoop",
    "text": "Hadoop\nDeveloped in 2006 to process data with the MapReduce programming model Dean and Ghemawat (2008), with the “big data” processing idea.\n. . .\nMain components of Apache Hadoop:\n\nHadoop Distributed File System (HDFS): Manages large data sets running on commodity hardware (with data replication)\nYet Another Resource Negotiator (YARN)\nHadoop MapReduce: Splits data into blocks, distributes across different nodes, then runs task on each block in parallel\nHadoop Common (Hadoop Core): Common libraries and utilities\n\n. . .\nWhy Hadoop is not enough?\n\nHadoop utilizes external storage. Read/write latency is high.\nAll codes must be restricted to a map and reduce constructs.",
    "crumbs": [
      "Parallelization of computing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to parallelized computing</span>"
    ]
  },
  {
    "objectID": "05-spark.html#spark",
    "href": "05-spark.html#spark",
    "title": "Intro to parallelized computing",
    "section": "Spark",
    "text": "Spark\nMain components:\n\nSpark Core: Execution engine for resource coordination\nSpark SQL\nMachine Learning Library (MLlib)\nSpark Streaming: Capability for processing streaming data\nGraphX: Capability for processing graph-based data\n\n. . .\nSpark essentially tackles only where Hadoop does not perform well, by using an in-memory distributed computing engine,\n\nmaking use of Resilient Distributed Dataset (RDD)\nemploying dynamic RAM in processing\n\n\n\n\nMapReduce schema from Tian and Zhao (2015).\n\n\n\n\n\n\n\n\nDean, Jeffrey, and Sanjay Ghemawat. 2008. “MapReduce: Simplified Data Processing on Large Clusters.” Communications of the ACM 51 (1): 107–13.\n\n\nReis, Joe, and Matt Housley. 2022. Fundamentals of Data Engineering. O’Reilly Media, Inc.\n\n\nTian, Wenhong, and Yong Zhao. 2015. “9 - Energy Efficiency Scheduling in Hadoop.” In Optimized Cloud Resource Management and Scheduling, edited by Wenhong Tian and Yong Zhao, 179–204. Boston: Morgan Kaufmann. https://doi.org/https://doi.org/10.1016/B978-0-12-801476-9.00009-4.",
    "crumbs": [
      "Parallelization of computing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Intro to parallelized computing</span>"
    ]
  },
  {
    "objectID": "0a-jupyter-ec2.html",
    "href": "0a-jupyter-ec2.html",
    "title": "Setting up EC2 instance for jupyter notebook",
    "section": "",
    "text": "Reconnect to EC2 Jupyter notebook\nImportant: Whenever you are not running anything (for more than 15 minutes), you should stop the EC2 instance on the AWS Instances page.",
    "crumbs": [
      "Useful guides",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Setting up EC2 instance for jupyter notebook</span>"
    ]
  },
  {
    "objectID": "0a-jupyter-ec2.html#reconnect-to-ec2-jupyter-notebook",
    "href": "0a-jupyter-ec2.html#reconnect-to-ec2-jupyter-notebook",
    "title": "Setting up EC2 instance for jupyter notebook",
    "section": "",
    "text": "Start your EC2 instance from https://nu-sso.awsapps.com/start/. (The one you have a private key for.)\nOnce the EC2 instance is running, connect to the instance via SSH\n\nUnder the SSH client tab, you will see the instructions to connect to the EC2 instance via SSH on your console.\n\nIf your EC2 console shows (base) before your cursor, it indicates that conda environment is already running.\n\nIf not, you have to run eval \"$($CONDA_PATH/bin/conda shell.bash hook)\" to activate the conda environment.\n\nSSH into Jupyter notebook on EC2 (as in step 7 above.)",
    "crumbs": [
      "Useful guides",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Setting up EC2 instance for jupyter notebook</span>"
    ]
  },
  {
    "objectID": "99-references.html",
    "href": "99-references.html",
    "title": "References",
    "section": "",
    "text": "Dean, Jeffrey, and Sanjay Ghemawat. 2008. “MapReduce: Simplified\nData Processing on Large Clusters.” Communications of the\nACM 51 (1): 107–13.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n“Bert: Pre-Training of Deep Bidirectional Transformers for\nLanguage Understanding.” In Proceedings of the 2019\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), 4171–86.\n\n\nHastie, Trevor, Rahul Mazumder, Jason D Lee, and Reza Zadeh. 2015.\n“Matrix Completion and Low-Rank SVD via Fast Alternating Least\nSquares.” The Journal of Machine Learning Research 16\n(1): 3367–3402.\n\n\nHu, Edward J, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,\nShean Wang, Lu Wang, Weizhu Chen, et al. 2022. “Lora: Low-Rank\nAdaptation of Large Language Models.” ICLR 1 (2): 3.\n\n\nReis, Joe, and Matt Housley. 2022. Fundamentals of Data\nEngineering. O’Reilly Media, Inc.\n\n\nTian, Wenhong, and Yong Zhao. 2015. “9 - Energy Efficiency\nScheduling in Hadoop.” In Optimized Cloud Resource Management\nand Scheduling, edited by Wenhong Tian and Yong Zhao, 179–204.\nBoston: Morgan Kaufmann. https://doi.org/https://doi.org/10.1016/B978-0-12-801476-9.00009-4.",
    "crumbs": [
      "References"
    ]
  }
]