{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wMoMrQ-je41K"
   },
   "outputs": [],
   "source": [
    "# spark setup\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jdk-11.0.2\"\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\Program Files\\Spark\\spark-3.5.5-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DC97uPhMe5cn"
   },
   "outputs": [],
   "source": [
    "# findspark helps locate the environment variables\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7yr9_JzZwQ6"
   },
   "source": [
    "# `agnews` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "S51T-4qYZ0LU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 12 29.3M   12 3759k    0     0  5229k      0  0:00:05 --:--:--  0:00:05 5236k\n",
      "100 29.3M  100 29.3M    0     0  20.8M      0  0:00:01  0:00:01 --:--:-- 20.8M\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews.csv -O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WOEde4yY8_u"
   },
   "source": [
    "# Pipelining with PySpark MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rWgYKCtGY8vy"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline # pipeline to transform data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PC9bF4RLQvsn"
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"AG news\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xhcOjvnyqOUf"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = spark.read.csv(\"agnews.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "g5WQzZTcqtSZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|Class Index|               Title|         Description|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          3|Wall St. Bears Cl...|Reuters - Short-s...|\n",
      "|          3|Carlyle Looks Tow...|Reuters - Private...|\n",
      "|          3|Oil and Economy C...|Reuters - Soaring...|\n",
      "|          3|Iraq Halts Oil Ex...|Reuters - Authori...|\n",
      "|          3|Oil prices soar t...|AFP - Tearaway wo...|\n",
      "|          3|Stocks End Up, Bu...|Reuters - Stocks ...|\n",
      "|          3|Money Funds Fell ...|AP - Assets of th...|\n",
      "|          3|Fed minutes show ...|USATODAY.com - Re...|\n",
      "|          3|Safety Net (Forbe...|\"Forbes.com - Aft...|\n",
      "|          3|Wall St. Bears Cl...| NEW YORK (Reuter...|\n",
      "|          3|Oil and Economy C...| NEW YORK (Reuter...|\n",
      "|          3|No Need for OPEC ...| TEHRAN (Reuters)...|\n",
      "|          3|Non-OPEC Nations ...| JAKARTA (Reuters...|\n",
      "|          3|Google IPO Auctio...| WASHINGTON/NEW Y...|\n",
      "|          3|Dollar Falls Broa...| NEW YORK (Reuter...|\n",
      "|          3|Rescuing an Old S...|If you think you ...|\n",
      "|          3|Kids Rule for Bac...|The purchasing po...|\n",
      "|          3|In a Down Market,...|There is little c...|\n",
      "|          3|US trade deficit ...|The US trade defi...|\n",
      "|          3|Shell 'could be t...|Oil giant Shell c...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGqkLxIDt8L8"
   },
   "source": [
    "# Arrange columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8jgV-jK9snHr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|                text|\n",
      "+-----+--------------------+\n",
      "|    3|Wall St. Bears Cl...|\n",
      "|    3|Carlyle Looks Tow...|\n",
      "|    3|Oil and Economy C...|\n",
      "|    3|Iraq Halts Oil Ex...|\n",
      "|    3|Oil prices soar t...|\n",
      "|    3|Stocks End Up, Bu...|\n",
      "|    3|Money Funds Fell ...|\n",
      "|    3|Fed minutes show ...|\n",
      "|    3|Safety Net (Forbe...|\n",
      "|    3|Wall St. Bears Cl...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws, col # to concatinate cols\n",
    "\n",
    "# Renaming 'Class Index' col to 'label'\n",
    "df = df.withColumnRenamed('Class Index', 'label')\n",
    "\n",
    "# Add a new column 'text' by concatinating 'Title' and 'Description'\n",
    "df = df.withColumn(\"text\", concat_ws(\" \", \"Title\", 'Description'))\n",
    "\n",
    "# Remove old text columns\n",
    "df = df.select('label', 'text')\n",
    "\n",
    "# Shows top 10 rows\n",
    "df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4s8B9lDuSZm"
   },
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iluZMjpnuNnT"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer # tokenizer\n",
    "\n",
    "# convert sentences to list of words\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# applies tokenizer to df\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "df.select(['label','text', 'words']).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zujVzF9vGQ4"
   },
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwqRSHqtu12B"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# remove stopwords\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "# adds a column 'filtered' to df without stopwords\n",
    "df = stopwords_remover.transform(df)\n",
    "\n",
    "df.select(['label','text', 'words', 'filtered']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lB7I_H3vSKJ"
   },
   "source": [
    "# Term frequency, Inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vfAqoeZvRwV"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# Calculate term frequency in each article\n",
    "hashing_tf = HashingTF(inputCol=\"filtered\",\n",
    "                       outputCol=\"raw_features\",\n",
    "                       numFeatures=16384)\n",
    "\n",
    "# adds raw tf features to df\n",
    "featurized_data = hashing_tf.transform(df)\n",
    "\n",
    "featurized_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0bYYaoZvihR"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "# inverse document frequency\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "idf_vectorizer = idf.fit(featurized_data)\n",
    "\n",
    "# converting text to vectors\n",
    "rescaled_data = idf_vectorizer.transform(featurized_data)\n",
    "\n",
    "rescaled_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwXEZpVswcO_"
   },
   "outputs": [],
   "source": [
    "rescaled_data.select('raw_features').show(1, truncate=False)\n",
    "rescaled_data.select('features').show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "macNK1smxX5s"
   },
   "source": [
    "# Training a multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeLPS8HwxXXN"
   },
   "outputs": [],
   "source": [
    "(train, test) = rescaled_data.randomSplit([0.75, 0.25])\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01Ml9cUlww1m"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol='features',\n",
    "                        labelCol='label',\n",
    "                        family=\"multinomial\",\n",
    "                        regParam=0.3,\n",
    "                        elasticNetParam=0,\n",
    "                        maxIter=20)\n",
    "\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpkPVcdnz0Mi"
   },
   "source": [
    "# Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t880vBmqzswD"
   },
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsqzIgId1vf4"
   },
   "outputs": [],
   "source": [
    "predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVNCsmpw0Onm"
   },
   "outputs": [],
   "source": [
    "predictions.select(\"text\", 'probability').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_0GCx5B1oxt"
   },
   "outputs": [],
   "source": [
    "predictions.select(\"text\", 'probability', 'prediction', 'label').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9sCKrPef3MwT"
   },
   "outputs": [],
   "source": [
    "# accuracy flag\n",
    "predictions = predictions.withColumn('correctFlag', (col('label') == col('prediction')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7PbHj9D0HUP"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.types import FloatType\n",
    "# accuracy\n",
    "predictions.select(avg(col('correctFlag').cast(FloatType())).alias('accuracy')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaiuTIx53cyq"
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# labels = [\"World\", \"Sports\", \"Business\",\"Science\"]\n",
    "\n",
    "# take only the predictions\n",
    "preds_and_labels = predictions.select(['prediction','label']) \\\n",
    "                              .withColumn('label', col('label') \\\n",
    "                              .cast(FloatType())) \\\n",
    "                              .orderBy('prediction')\n",
    "\n",
    "# generate confusion matrix counts\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNtcwxpY4REC"
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjK_PlwD4fqi"
   },
   "source": [
    "# Pipelining, from start to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYlA_fev4iV4"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = spark.read.csv(\"agnews.csv\", inferSchema=True, header=True)\n",
    "\n",
    "def arrangeColumns(df):\n",
    "  # Renaming 'Class Index' col to 'label'\n",
    "  df = df.withColumnRenamed('Class Index', 'label')\n",
    "\n",
    "  # Add a new column 'text' by joining 'Title' and 'Description'\n",
    "  df = df.withColumn(\"text\", concat_ws(\" \", \"Title\", 'Description'))\n",
    "\n",
    "  # Select new text feature and labels\n",
    "  df = df.select('label', 'text')\n",
    "  return df\n",
    "\n",
    "df = arrangeColumns(df)\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# stopwords\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "# term frequency\n",
    "hashing_tf = HashingTF(inputCol=\"filtered\",\n",
    "                       outputCol=\"raw_features\",\n",
    "                       numFeatures=16384)\n",
    "\n",
    "# Inverse Document Frequency\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# model\n",
    "lr = LogisticRegression(featuresCol='features',\n",
    "                        labelCol='label',\n",
    "                        family=\"multinomial\",\n",
    "                        regParam=0.3,\n",
    "                        elasticNetParam=0,\n",
    "                        maxIter=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9_LHoAR5Cc-"
   },
   "outputs": [],
   "source": [
    "# Put everything in pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer,\n",
    "                            stopwords_remover,\n",
    "                            hashing_tf,\n",
    "                            idf,\n",
    "                            lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(df)\n",
    "\n",
    "# transform add train\n",
    "dataset = pipelineFit.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zuhQmk46PjF"
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "dataset = dataset.withColumn('correctFlag', (col('label') == col('prediction')))\n",
    "dataset.select(avg(col('correctFlag').cast(FloatType())).alias('accuracy')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUsH_9a36bX9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOAVBCKK/HPnCbpIGlAzRAR",
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
