{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (EX) News article processing (with ML pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7yr9_JzZwQ6"
   },
   "source": [
    "# `agnews` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S51T-4qYZ0LU"
   },
   "outputs": [],
   "source": [
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews.csv -O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WOEde4yY8_u"
   },
   "source": [
    "# Pipelining with PySpark MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWgYKCtGY8vy"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline # pipeline to transform data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PC9bF4RLQvsn"
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"AG news\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhcOjvnyqOUf"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGqkLxIDt8L8"
   },
   "source": [
    "# Arrange columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jgV-jK9snHr"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, col # to concatinate cols\n",
    "\n",
    "# renaming 'Class Index' col to 'label'\n",
    "\n",
    "# concatenating texts\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4s8B9lDuSZm"
   },
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iluZMjpnuNnT"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer # tokenizer\n",
    "\n",
    "# convert sentences to list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zujVzF9vGQ4"
   },
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwqRSHqtu12B"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lB7I_H3vSKJ"
   },
   "source": [
    "# Term frequency, Inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vfAqoeZvRwV"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# calculate term frequency in each article (row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0bYYaoZvihR"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "# inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwXEZpVswcO_"
   },
   "outputs": [],
   "source": [
    "rescaled_data.select('raw_features').show(2, truncate=False)\n",
    "rescaled_data.select('features').show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "macNK1smxX5s"
   },
   "source": [
    "# Training a multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeLPS8HwxXXN"
   },
   "outputs": [],
   "source": [
    "# split data into training and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01Ml9cUlww1m"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpkPVcdnz0Mi"
   },
   "source": [
    "# Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t880vBmqzswD"
   },
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaiuTIx53cyq"
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# labels = [\"World\", \"Sports\", \"Business\",\"Science\"]\n",
    "\n",
    "# take only the predictions\n",
    "preds_and_labels = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNtcwxpY4REC"
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "metrics = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjK_PlwD4fqi"
   },
   "source": [
    "# Pipelining, from start to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYlA_fev4iV4"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = spark.read.csv(\"agnews.csv\", inferSchema=True, header=True)\n",
    "\n",
    "def arrangeColumns(df):\n",
    "  # Renaming 'Class Index' col to 'label'\n",
    "  df = df.withColumnRenamed('Class Index', 'label')\n",
    "\n",
    "  # Add a new column 'text' by joining 'Title' and 'Description'\n",
    "  df = df.withColumn(\"text\", concat_ws(\" \", \"Title\", 'Description'))\n",
    "\n",
    "  # Select new text feature and labels\n",
    "  df = df.select('label', 'text')\n",
    "  return df\n",
    "\n",
    "df = arrangeColumns(df)\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# stopwords\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "# term frequency\n",
    "hashing_tf = HashingTF(inputCol=\"filtered\",\n",
    "                       outputCol=\"raw_features\",\n",
    "                       numFeatures=16384)\n",
    "\n",
    "# Inverse Document Frequency\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# model\n",
    "lr = LogisticRegression(featuresCol='features',\n",
    "                        labelCol='label',\n",
    "                        family=\"multinomial\",\n",
    "                        regParam=0.3,\n",
    "                        elasticNetParam=0,\n",
    "                        maxIter=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9_LHoAR5Cc-"
   },
   "outputs": [],
   "source": [
    "# Put everything in pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer,\n",
    "                            stopwords_remover,\n",
    "                            hashing_tf,\n",
    "                            idf,\n",
    "                            lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(df)\n",
    "\n",
    "# transform and train\n",
    "dataset = pipelineFit.transform(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOAVBCKK/HPnCbpIGlAzRAR",
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
