{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (EX) News article processing (with ML pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMoMrQ-je41K"
   },
   "outputs": [],
   "source": [
    "# spark setup, only if you need to specify your paths\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jdk-11.0.2\"\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\Program Files\\Spark\\spark-3.5.5-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DC97uPhMe5cn"
   },
   "outputs": [],
   "source": [
    "# findspark helps locate the environment variables\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7yr9_JzZwQ6"
   },
   "source": [
    "# `agnews` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S51T-4qYZ0LU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0 29.3M    0  222k    0     0   137k      0  0:03:38  0:00:01  0:03:37  137k\n",
      " 10 29.3M   10 3146k    0     0  1208k      0  0:00:24  0:00:02  0:00:22 1208k\n",
      " 19 29.3M   19 5975k    0     0  1655k      0  0:00:18  0:00:03  0:00:15 1655k\n",
      " 26 29.3M   26 7938k    0     0  1722k      0  0:00:17  0:00:04  0:00:13 1722k\n",
      " 32 29.3M   32 9712k    0     0  1729k      0  0:00:17  0:00:05  0:00:12 2305k\n",
      " 35 29.3M   35 10.3M    0     0  1608k      0  0:00:18  0:00:06  0:00:12 2085k\n",
      " 43 29.3M   43 12.6M    0     0  1699k      0  0:00:17  0:00:07  0:00:10 1955k\n",
      " 49 29.3M   49 14.5M    0     0  1728k      0  0:00:17  0:00:08  0:00:09 1780k\n",
      " 55 29.3M   55 16.4M    0     0  1750k      0  0:00:17  0:00:09  0:00:08 1775k\n",
      " 62 29.3M   62 18.4M    0     0  1782k      0  0:00:16  0:00:10  0:00:06 1841k\n",
      " 69 29.3M   69 20.3M    0     0  1792k      0  0:00:16  0:00:11  0:00:05 2034k\n",
      " 76 29.3M   76 22.3M    0     0  1818k      0  0:00:16  0:00:12  0:00:04 1999k\n",
      " 80 29.3M   80 23.7M    0     0  1784k      0  0:00:16  0:00:13  0:00:03 1882k\n",
      " 87 29.3M   87 25.7M    0     0  1802k      0  0:00:16  0:00:14  0:00:02 1903k\n",
      " 95 29.3M   95 28.1M    0     0  1846k      0  0:00:16  0:00:15  0:00:01 1982k\n",
      "100 29.3M  100 29.3M    0     0  1794k      0  0:00:16  0:00:16 --:--:-- 1800k\n",
      "100 29.3M  100 29.3M    0     0  1794k      0  0:00:16  0:00:16 --:--:-- 1721k\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews.csv -O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WOEde4yY8_u"
   },
   "source": [
    "# Pipelining with PySpark MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWgYKCtGY8vy"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline # pipeline to transform data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PC9bF4RLQvsn"
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"AG news\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhcOjvnyqOUf"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = spark.read.csv(\"agnews.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5WQzZTcqtSZ"
   },
   "outputs": [],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGqkLxIDt8L8"
   },
   "source": [
    "# Arrange columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jgV-jK9snHr"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, col # to concatinate cols\n",
    "\n",
    "# renaming 'Class Index' col to 'label'\n",
    "df = df.withColumnRenamed('Class Index', 'label')\n",
    "\n",
    "# concatenating texts\n",
    "df = df.withColumn('text', concat_ws(\" \", \"Title\", \"Description\"))\n",
    "\n",
    "df = df.select('label', 'text')\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4s8B9lDuSZm"
   },
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iluZMjpnuNnT"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer # tokenizer\n",
    "\n",
    "# convert sentences to list of words\n",
    "tokenizer = RegexTokenizer(inputCol='text', outputCol='words', pattern=\"\\\\W\")\n",
    "\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "df.select(['label', 'text', 'words']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zujVzF9vGQ4"
   },
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwqRSHqtu12B"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# remove stopwords\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "df = stopwords_remover.transform(df)\n",
    "\n",
    "\n",
    "df.select(['label', 'text', 'words', 'filtered']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lB7I_H3vSKJ"
   },
   "source": [
    "# Term frequency, Inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vfAqoeZvRwV"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# calculate term frequency in each article (row)\n",
    "\n",
    "hashing_tf = HashingTF(inputCol=\"filtered\", outputCol=\"raw_features\",\n",
    "                       numFeatures=16384)\n",
    "\n",
    "featurized_data = hashing_tf.transform(df)\n",
    "\n",
    "featurized_data.select('raw_features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0bYYaoZvihR"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "# inverse document frequency\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "idf_vectorizer = idf.fit(featurized_data)\n",
    "\n",
    "rescaled_data = idf_vectorizer.transform(featurized_data)\n",
    "\n",
    "rescaled_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwXEZpVswcO_"
   },
   "outputs": [],
   "source": [
    "rescaled_data.select('raw_features').show(2, truncate=False)\n",
    "rescaled_data.select('features').show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "macNK1smxX5s"
   },
   "source": [
    "# Training a multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeLPS8HwxXXN"
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "(train, test) = rescaled_data.randomSplit([0.75, 0.25], seed=42)\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01Ml9cUlww1m"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features',\n",
    "                        labelCol='label',\n",
    "                        family='multinomial',\n",
    "                        regParam=0.3,\n",
    "                        elasticNetParam=0,\n",
    "                        maxIter=20)\n",
    "\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpkPVcdnz0Mi"
   },
   "source": [
    "# Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t880vBmqzswD"
   },
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select('probability', 'prediction').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.withColumn('correctFlag', (col('label') == col('prediction')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "predictions.select(avg(col('correctFlag').cast(FloatType())).alias('acurracy')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaiuTIx53cyq"
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# labels = [\"World\", \"Sports\", \"Business\",\"Science\"]\n",
    "\n",
    "# take only the predictions\n",
    "preds_and_labels = predictions.select(['prediction','label']) \\\n",
    "                              .withColumn('label', col('label') \\\n",
    "                              .cast(FloatType())) \\\n",
    "                              .orderBy('prediction')\n",
    "\n",
    "\n",
    "preds_and_labels.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNtcwxpY4REC"
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "metrics = MulticlassMetrics(predictionAndLabels=preds_and_labels.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjK_PlwD4fqi"
   },
   "source": [
    "# Pipelining, from start to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYlA_fev4iV4"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = spark.read.csv(\"agnews.csv\", inferSchema=True, header=True)\n",
    "\n",
    "def arrangeColumns(df):\n",
    "  # Renaming 'Class Index' col to 'label'\n",
    "  df = df.withColumnRenamed('Class Index', 'label')\n",
    "\n",
    "  # Add a new column 'text' by joining 'Title' and 'Description'\n",
    "  df = df.withColumn(\"text\", concat_ws(\" \", \"Title\", 'Description'))\n",
    "\n",
    "  # Select new text feature and labels\n",
    "  df = df.select('label', 'text')\n",
    "  return df\n",
    "\n",
    "df = arrangeColumns(df)\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# stopwords\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "# term frequency\n",
    "hashing_tf = HashingTF(inputCol=\"filtered\",\n",
    "                       outputCol=\"raw_features\",\n",
    "                       numFeatures=16384)\n",
    "\n",
    "# Inverse Document Frequency\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# model\n",
    "lr = LogisticRegression(featuresCol='features',\n",
    "                        labelCol='label',\n",
    "                        family=\"multinomial\",\n",
    "                        regParam=0.3,\n",
    "                        elasticNetParam=0,\n",
    "                        maxIter=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9_LHoAR5Cc-"
   },
   "outputs": [],
   "source": [
    "# Put everything in pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer,\n",
    "                            stopwords_remover,\n",
    "                            hashing_tf,\n",
    "                            idf,\n",
    "                            lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(df)\n",
    "\n",
    "# transform and train\n",
    "dataset = pipelineFit.transform(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOAVBCKK/HPnCbpIGlAzRAR",
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
